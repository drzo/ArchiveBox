{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXyLvrHfQKT9VJt0Q7D3ib",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drzo/ArchiveBox/blob/main/Tiny_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxt6YIof_rKC",
        "outputId": "feefb3da-1d6b-4a5f-cb70-471a8e244945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "310"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a tiny transformer-like model with around 1000 parameters\n",
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size=10, hidden_size=5, num_attention_heads=1, num_hidden_layers=1, intermediate_size=5):\n",
        "        super(TinyTransformer, self).__init__()\n",
        "        # Embedding layer\n",
        "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        # Transformer encoder layer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_attention_heads, dim_feedforward=intermediate_size)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_hidden_layers)\n",
        "\n",
        "        # Output layer (simple linear classifier for vocab prediction)\n",
        "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # Embed the input tokens\n",
        "        embeddings = self.embeddings(input_ids)\n",
        "\n",
        "        # Pass through the transformer encoder\n",
        "        encoder_output = self.encoder(embeddings)\n",
        "\n",
        "        # Generate predictions\n",
        "        logits = self.fc_out(encoder_output)\n",
        "        return logits\n",
        "\n",
        "# Initialize the tiny transformer model\n",
        "tiny_model = TinyTransformer(vocab_size=10, hidden_size=5, num_attention_heads=1, num_hidden_layers=1, intermediate_size=5)\n",
        "\n",
        "# Check the number of parameters\n",
        "param_count = sum(p.numel() for p in tiny_model.parameters())\n",
        "param_count  # Show the number of parameters\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import PreTrainedModel, PretrainedConfig\n",
        "\n",
        "# Define a tiny model config for ~1000 parameters using a small transformer\n",
        "class TinyConfig(PretrainedConfig):\n",
        "    def __init__(self, vocab_size=10, hidden_size=5, num_attention_heads=1, num_hidden_layers=1, intermediate_size=5, max_position_embeddings=10, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "\n",
        "class TinyModel(PreTrainedModel):\n",
        "    config_class = TinyConfig\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=config.hidden_size, nhead=config.num_attention_heads, dim_feedforward=config.intermediate_size)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=config.num_hidden_layers)\n",
        "        self.fc_out = nn.Linear(config.hidden_size, config.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embeddings = self.embeddings(input_ids)\n",
        "        encoder_output = self.encoder(embeddings)\n",
        "        logits = self.fc_out(encoder_output)\n",
        "        return logits\n",
        "\n",
        "# Create a tiny model config with only ~1000 parameters\n",
        "config = TinyConfig(vocab_size=10, hidden_size=5, num_attention_heads=1, num_hidden_layers=1, intermediate_size=5)\n",
        "tiny_model = TinyModel(config)\n",
        "\n",
        "# Check the number of parameters\n",
        "param_count = sum(p.numel() for p in tiny_model.parameters())\n",
        "param_count\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28xUszXCApay",
        "outputId": "9ce1cbf4-2a9f-494a-c340-ee3648123507"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "310"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a tiny transformer-like model with ~1000 parameters\n",
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size=10, hidden_size=5, num_attention_heads=1, num_hidden_layers=1, intermediate_size=5):\n",
        "        super(TinyTransformer, self).__init__()\n",
        "        # Embedding layer\n",
        "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        # Transformer encoder layer with batch_first=True for better performance\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_attention_heads, dim_feedforward=intermediate_size, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_hidden_layers)\n",
        "\n",
        "        # Output layer (simple linear classifier for vocab prediction)\n",
        "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # Embed the input tokens\n",
        "        embeddings = self.embeddings(input_ids)\n",
        "\n",
        "        # Pass through the transformer encoder\n",
        "        encoder_output = self.encoder(embeddings)\n",
        "\n",
        "        # Generate predictions\n",
        "        logits = self.fc_out(encoder_output)\n",
        "        return logits\n",
        "\n",
        "# Initialize the tiny transformer model\n",
        "tiny_model = TinyTransformer(vocab_size=10, hidden_size=5, num_attention_heads=1, num_hidden_layers=1, intermediate_size=5)\n",
        "\n",
        "# Check the number of parameters\n",
        "param_count = sum(p.numel() for p in tiny_model.parameters())\n",
        "print(f\"Tiny model parameter count: {param_count}\")\n",
        "\n",
        "# Test with a random input\n",
        "input_ids = torch.randint(0, 10, (2, 5))  # Example input (batch_size=2, seq_len=5)\n",
        "logits = tiny_model(input_ids)\n",
        "\n",
        "print(f\"Logits output: {logits}\")\n",
        "\n",
        "# Save the model to a file\n",
        "torch.save(tiny_model.state_dict(), \"tiny_transformer.pth\")\n",
        "\n",
        "# Download the saved model file\n",
        "import shutil\n",
        "shutil.move(\"tiny_transformer.pth\", \"/mnt/data/tiny_transformer.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "oRGi6nq6AuuB",
        "outputId": "73933915-423b-4001-828b-2217064c8e36"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiny model parameter count: 310\n",
            "Logits output: tensor([[[ 0.5601, -0.2599, -1.2039, -1.3598, -0.2848,  0.7035, -0.2403,\n",
            "          -0.0189,  0.0961, -0.1059],\n",
            "         [ 0.4575, -1.0482,  0.2084,  0.3093, -0.3500, -1.0421,  0.5726,\n",
            "           0.8388,  1.3241,  0.1062],\n",
            "         [ 0.5679,  0.1536, -0.6225, -0.7665,  0.4661,  0.3574, -0.0095,\n",
            "          -0.1015, -1.0091,  0.4691],\n",
            "         [ 0.3031, -1.0941,  0.7482,  0.5749, -0.1606, -1.7083,  0.9077,\n",
            "           1.0214,  0.7131,  0.3250],\n",
            "         [ 1.6425, -0.9661, -0.0105, -1.3926,  0.4635,  0.0649, -0.3751,\n",
            "           0.7639, -1.0450,  0.3279]],\n",
            "\n",
            "        [[-1.1167,  0.8018, -0.8326,  1.0902, -0.2196, -0.3823,  1.0223,\n",
            "          -0.5940,  1.0485,  0.3720],\n",
            "         [-0.4901,  0.0317, -0.1806, -0.1316, -0.0762, -0.9531,  0.8817,\n",
            "           0.0846, -0.6760,  0.3716],\n",
            "         [ 0.3822, -0.9963,  0.0968,  0.2390, -0.4120, -0.9724,  0.5654,\n",
            "           0.7755,  1.3448,  0.0628],\n",
            "         [ 1.7766, -1.6772,  0.8605, -0.6716,  0.3457, -0.9742,  0.0028,\n",
            "           1.4332, -0.3225,  0.3611],\n",
            "         [ 0.4008,  0.2225, -0.6313, -0.7497,  0.4241,  0.2637,  0.0863,\n",
            "          -0.1463, -1.0980,  0.4690]]], grad_fn=<ViewBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/mnt/data/tiny_transformer.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tiny_transformer.pth' -> '/mnt/data/tiny_transformer.pth'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ff6b6f3059bc>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Download the saved model file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tiny_transformer.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/mnt/data/tiny_transformer.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m             \u001b[0mcopy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m                     \u001b[0;31m# macOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0m_HAS_FCOPYFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/tiny_transformer.pth'"
          ]
        }
      ]
    },
    {
      "source": [
        "# Download the saved model file\n",
        "import shutil\n",
        "!mkdir /mnt/data # Create the directory\n",
        "shutil.move(\"tiny_transformer.pth\", \"/mnt/data/tiny_transformer.pth\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RYz1LhCbCNOc",
        "outputId": "ea413674-7970-4bf2-ecad-6c13bd77b3db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/mnt/data/tiny_transformer.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "01UQBxNlERgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the model to ONNX format\n",
        "dummy_input = torch.randint(0, 10, (2, 5))  # Example input (batch_size=2, seq_len=5)\n",
        "torch.onnx.export(tiny_model, dummy_input, \"tiny_transformer.onnx\", input_names=['input'], output_names=['output'])\n",
        "\n",
        "# Download the ONNX model\n",
        "shutil.move(\"tiny_transformer.onnx\", \"/mnt/data/tiny_transformer.onnx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "ayV3DiS2B-Zo",
        "outputId": "5381162f-5d4f-45c0-b175-69ba176b23f9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OnnxExporterError",
          "evalue": "Module onnx is not installed!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/onnx_proto_utils.py\u001b[0m in \u001b[0;36m_add_onnxscript_fn\u001b[0;34m(model_bytes, custom_opsets)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'onnx'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOnnxExporterError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-bab82b6a4589>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Export the model to ONNX format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdummy_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Example input (batch_size=2, seq_len=5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiny_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tiny_transformer.onnx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Download the ONNX model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining, dynamo)\u001b[0m\n\u001b[1;32m    549\u001b[0m         )\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m     _export(\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1720\u001b[0m                 )\n\u001b[1;32m   1721\u001b[0m             \u001b[0;31m# insert function_proto into model_proto.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1722\u001b[0;31m             proto = onnx_proto_utils._add_onnxscript_fn(\n\u001b[0m\u001b[1;32m   1723\u001b[0m                 \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m                 \u001b[0mcustom_opsets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/onnx_proto_utils.py\u001b[0m in \u001b[0;36m_add_onnxscript_fn\u001b[0;34m(model_bytes, custom_opsets)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOnnxExporterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Module onnx is not installed!\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;31m# For > 2GB model, onnx.load_fromstring would fail. However, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOnnxExporterError\u001b[0m: Module onnx is not installed!"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install onnx # Install the missing onnx module\n",
        "# Export the model to ONNX format\n",
        "dummy_input = torch.randint(0, 10, (2, 5))  # Example input (batch_size=2, seq_len=5)\n",
        "torch.onnx.export(tiny_model, dummy_input, \"tiny_transformer.onnx\", input_names=['input'], output_names=['output'])\n",
        "\n",
        "# Download the ONNX model\n",
        "shutil.move(\"tiny_transformer.onnx\", \"/mnt/data/tiny_transformer.onnx\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "PoCgJEYiCe5-",
        "outputId": "34b72521-4658-4179-c1db-dc02d8402fdb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.16.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/mnt/data/tiny_transformer.onnx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "from onnx import numpy_helper\n",
        "import toml\n",
        "\n",
        "# Load the ONNX model\n",
        "model = onnx.load('tiny_transformer.onnx')\n",
        "graph = model.graph\n",
        "\n",
        "# Extract initializers (weights)\n",
        "initializers = {initializer.name: numpy_helper.to_array(initializer).tolist() for initializer in graph.initializer}\n",
        "\n",
        "# Example GGUF-like TOML structure\n",
        "gguf_data = {\n",
        "    \"nodes\": {\n",
        "        \"embedding\": {\n",
        "            \"type\": \"embedding\",\n",
        "            \"input_dim\": 10,\n",
        "            \"output_dim\": 5,\n",
        "            \"weights\": initializers.get('embedding_weights', [])\n",
        "        },\n",
        "        \"attention\": {\n",
        "            \"type\": \"self_attention\",\n",
        "            \"input_dim\": 5,\n",
        "            \"num_heads\": 1,\n",
        "            \"weights\": initializers.get('attention_weights', [])\n",
        "        },\n",
        "        \"output\": {\n",
        "            \"type\": \"linear\",\n",
        "            \"input_dim\": 5,\n",
        "            \"output_dim\": 10,\n",
        "            \"weights\": initializers.get('output_weights', [])\n",
        "        }\n",
        "    },\n",
        "    \"edges\": {\n",
        "        \"embedding_to_attention\": {\"from\": \"embedding\", \"to\": \"attention\"},\n",
        "        \"attention_to_output\": {\"from\": \"attention\", \"to\": \"output\"}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save the GGUF schema to a TOML file (simulating GGUF)\n",
        "with open(\"tiny_model_gguf_test.toml\", \"w\") as f:\n",
        "    toml.dump(gguf_data, f)\n",
        "\n",
        "# Provide a download link\n",
        "import shutil\n",
        "shutil.move(\"tiny_model_gguf_test.toml\", \"/mnt/data/tiny_model_gguf_test.toml\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "R0VAEiHZEU65",
        "outputId": "1a5dd62b-beb1-4ef3-cbd5-2ddfe4e7b57f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'tiny_transformer.onnx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-63e2169709b0>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the ONNX model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tiny_transformer.onnx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/onnx/__init__.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(f, format, load_external_data)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mLoaded\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0mModelProto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \"\"\"\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_serializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mload_external_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/onnx/__init__.py\u001b[0m in \u001b[0;36m_load_bytes\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreadable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tiny_transformer.onnx'"
          ]
        }
      ]
    },
    {
      "source": [
        "import onnx\n",
        "from onnx import numpy_helper\n",
        "import toml\n",
        "import shutil\n",
        "\n",
        "# Export the model to ONNX format (Ensure this step is done before loading)\n",
        "dummy_input = torch.randint(0, 10, (2, 5))  # Example input (batch_size=2, seq_len=5)\n",
        "# Assuming tiny_model is defined and available in the current scope\n",
        "torch.onnx.export(tiny_model, dummy_input, \"tiny_transformer.onnx\", input_names=['input'], output_names=['output'])\n",
        "\n",
        "# Load the ONNX model\n",
        "model = onnx.load('tiny_transformer.onnx')\n",
        "graph = model.graph\n",
        "\n",
        "# ... rest of your code ...\n",
        "\n",
        "# Move the toml file after it has been generated\n",
        "shutil.move(\"tiny_model_gguf_test.toml\", \"/mnt/data/tiny_model_gguf_test.toml\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "YfLFHcWKEcEU",
        "outputId": "75695f23-ec32-4e8e-debd-296b075aa9b6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'tiny_model_gguf_test.toml'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tiny_model_gguf_test.toml' -> '/mnt/data/tiny_model_gguf_test.toml'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-cd60589e0ea5>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Move the toml file after it has been generated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tiny_model_gguf_test.toml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/mnt/data/tiny_model_gguf_test.toml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m             \u001b[0mcopy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tiny_model_gguf_test.toml'"
          ]
        }
      ]
    },
    {
      "source": [
        "import onnx\n",
        "from onnx import numpy_helper\n",
        "import toml\n",
        "import shutil\n",
        "\n",
        "# Export the model to ONNX format (Ensure this step is done before loading)\n",
        "dummy_input = torch.randint(0, 10, (2, 5))  # Example input (batch_size=2, seq_len=5)\n",
        "# Assuming tiny_model is defined and available in the current scope\n",
        "torch.onnx.export(tiny_model, dummy_input, \"tiny_transformer.onnx\", input_names=['input'], output_names=['output'])\n",
        "\n",
        "# Load the ONNX model\n",
        "model = onnx.load('tiny_transformer.onnx')\n",
        "graph = model.graph\n",
        "\n",
        "# ... rest of your code ...\n",
        "\n",
        "# Save the GGUF schema to a TOML file (simulating GGUF)\n",
        "with open(\"tiny_model_gguf_test.toml\", \"w\") as f:\n",
        "    toml.dump(gguf_data, f)\n",
        "\n",
        "# Move the toml file after it has been generated\n",
        "shutil.move(\"tiny_model_gguf_test.toml\", \"/mnt/data/tiny_model_gguf_test.toml\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "twhfNospElFU",
        "outputId": "20493f36-aa4b-491f-a2d6-589db25ab8ad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gguf_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9e55e76f8bc8>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Save the GGUF schema to a TOML file (simulating GGUF)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tiny_model_gguf_test.toml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtoml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgguf_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Move the toml file after it has been generated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gguf_data' is not defined"
          ]
        }
      ]
    },
    {
      "source": [
        "import onnx\n",
        "from onnx import numpy_helper\n",
        "import toml\n",
        "import shutil\n",
        "\n",
        "# Export the model to ONNX format (Ensure this step is done before loading)\n",
        "dummy_input = torch.randint(0, 10, (2, 5))  # Example input (batch_size=2, seq_len=5)\n",
        "# Assuming tiny_model is defined and available in the current scope\n",
        "torch.onnx.export(tiny_model, dummy_input, \"tiny_transformer.onnx\", input_names=['input'], output_names=['output'])\n",
        "\n",
        "# Load the ONNX model\n",
        "model = onnx.load('tiny_transformer.onnx')\n",
        "graph = model.graph\n",
        "\n",
        "# Extract initializers (weights)\n",
        "initializers = {initializer.name: numpy_helper.to_array(initializer).tolist() for initializer in graph.initializer}\n",
        "\n",
        "# Example GGUF-like TOML structure\n",
        "gguf_data = { # This code was missing\n",
        "    \"nodes\": {\n",
        "        \"embedding\": {\n",
        "            \"type\": \"embedding\",\n",
        "            \"input_dim\": 10,\n",
        "            \"output_dim\": 5,\n",
        "            \"weights\": initializers.get('embedding_weights', [])\n",
        "        },\n",
        "        \"attention\": {\n",
        "            \"type\": \"self_attention\",\n",
        "            \"input_dim\": 5,\n",
        "            \"num_heads\": 1,\n",
        "            \"weights\": initializers.get('attention_weights', [])\n",
        "        },\n",
        "        \"output\": {\n",
        "            \"type\": \"linear\",\n",
        "            \"input_dim\": 5,\n",
        "            \"output_dim\": 10,\n",
        "            \"weights\": initializers.get('output_weights', [])\n",
        "        }\n",
        "    },\n",
        "    \"edges\": {\n",
        "        \"embedding_to_attention\": {\"from\": \"embedding\", \""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Dygw2z4iEsqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import onnx\n",
        "from onnx import numpy_helper\n",
        "import toml\n",
        "import shutil\n",
        "\n",
        "# Export the model to ONNX format (Ensure this step is done before loading)\n",
        "dummy_input = torch.randint(0, 10, (2, 5))  # Example input (batch_size=2, seq_len=5)\n",
        "# Assuming tiny_model is defined and available in the current scope\n",
        "torch.onnx.export(tiny_model, dummy_input, \"tiny_transformer.onnx\", input_names=['input'], output_names=['output'])\n",
        "\n",
        "# Load the ONNX model\n",
        "model = onnx.load('tiny_transformer.onnx')\n",
        "graph = model.graph\n",
        "\n",
        "# Extract initializers (weights)\n",
        "initializers = {initializer.name: numpy_helper.to_array(initializer).tolist() for initializer in graph.initializer}\n",
        "\n",
        "# Example GGUF-like TOML structure\n",
        "gguf_data = { # This code was missing\n",
        "    \"nodes\": {\n",
        "        \"embedding\": {\n",
        "            \"type\": \"embedding\",\n",
        "            \"input_dim\": 10,\n",
        "            \"output_dim\": 5,\n",
        "            \"weights\": initializers.get('embedding_weights', [])\n",
        "        },\n",
        "        \"attention\": {\n",
        "            \"type\": \"self_attention\",\n",
        "            \"input_dim\": 5,\n",
        "            \"num_heads\": 1,\n",
        "            \"weights\": initializers.get('attention_weights', [])\n",
        "        },\n",
        "        \"output\": {\n",
        "            \"type\": \"linear\",\n",
        "            \"input_dim\": 5,\n",
        "            \"output_dim\": 10,\n",
        "            \"weights\": initializers.get('output_weights', [])\n",
        "        }\n",
        "    },\n",
        "    \"edges\": {\n",
        "        \"embedding_to_attention\": {\"from\": \"embedding\", \"to\": \"attention\"},\n",
        "        \"attention_to_output\": {\"from\": \"attention\", \"to\": \"output\"}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save the GGUF schema to a TOML file (simulating GGUF)\n",
        "with open(\"tiny_model_gguf_test.toml\", \"w\") as f:\n",
        "    toml.dump(gguf_data, f)\n",
        "\n",
        "# Move the toml file after it has been generated\n",
        "shutil.move(\"tiny_model_gguf_test.toml\", \"/mnt/data/tiny_model_gguf_test.toml\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pOdJEGSAEuFG",
        "outputId": "38756ec4-8b03-46da-e26d-023fd3da5474"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/mnt/data/tiny_model_gguf_test.toml'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import toml\n",
        "\n",
        "# Load the saved PyTorch model\n",
        "tiny_model = TinyTransformer(vocab_size=10, hidden_size=5, num_attention_heads=1, num_hidden_layers=1, intermediate_size=5)\n",
        "tiny_model.load_state_dict(torch.load('tiny_transformer.pth'))\n",
        "\n",
        "# Extract the weights\n",
        "embedding_weights = tiny_model.embeddings.weight.detach().numpy().tolist()\n",
        "attention_weights = tiny_model.encoder.layers[0].self_attn.in_proj_weight.detach().numpy().tolist()  # Query, Key, Value projection weights\n",
        "output_weights = tiny_model.fc_out.weight.detach().numpy().tolist()\n",
        "\n",
        "# Update the GGUF-like structure with the extracted weights\n",
        "gguf_data = {\n",
        "    \"nodes\": {\n",
        "        \"embedding\": {\n",
        "            \"type\": \"embedding\",\n",
        "            \"input_dim\": 10,\n",
        "            \"output_dim\": 5,\n",
        "            \"weights\": embedding_weights\n",
        "        },\n",
        "        \"attention\": {\n",
        "            \"type\": \"self_attention\",\n",
        "            \"input_dim\": 5,\n",
        "            \"num_heads\": 1,\n",
        "            \"weights\": attention_weights\n",
        "        },\n",
        "        \"output\": {\n",
        "            \"type\": \"linear\",\n",
        "            \"input_dim\": 5,\n",
        "            \"output_dim\": 10,\n",
        "            \"weights\": output_weights\n",
        "        }\n",
        "    },\n",
        "    \"edges\": {\n",
        "        \"embedding_to_attention\": {\"from\": \"embedding\", \"to\": \"attention\"},\n",
        "        \"attention_to_output\": {\"from\": \"attention\", \"to\": \"output\"}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save the updated GGUF-like TOML structure with weights\n",
        "with open(\"tiny_model_gguf_with_weights.toml\", \"w\") as f:\n",
        "    toml.dump(gguf_data, f)\n",
        "\n",
        "# Move the updated TOML file for download\n",
        "import shutil\n",
        "shutil.move(\"tiny_model_gguf_with_weights.toml\", \"/mnt/data/tiny_model_gguf_with_weights.toml\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "ga-ccPtIG8aG",
        "outputId": "a77a24bf-722b-4c89-b932-efd0d97017b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "<ipython-input-13-bf3af9510411>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  tiny_model.load_state_dict(torch.load('tiny_transformer.pth'))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'tiny_transformer.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-bf3af9510411>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the saved PyTorch model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtiny_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTinyTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintermediate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtiny_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tiny_transformer.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Extract the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tiny_transformer.pth'"
          ]
        }
      ]
    },
    {
      "source": [
        "import torch\n",
        "import toml\n",
        "\n",
        "# Load the saved PyTorch model\n",
        "tiny_model = TinyTransformer(vocab_size=10, hidden_size=5, num_attention_heads=1, num_hidden_layers=1, intermediate_size=5)\n",
        "# Check if the file exists. If not, provide instructions or handle the exception\n",
        "try:\n",
        "    tiny_model.load_state_dict(torch.load('/mnt/data/tiny_transformer.pth'))\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'tiny_transformer.pth' not found. Make sure the file exists in the correct directory or provide the correct path.\")\n",
        "    # You can add more comprehensive error handling or instructions here.\n",
        "\n",
        "# Extract the weights\n",
        "embedding_weights = tiny_model.embeddings.weight.detach().numpy().tolist()\n",
        "attention_weights = tiny_model.encoder.layers[0].self_attn.in_proj_weight.detach().numpy().tolist()  # Query, Key, Value projection weights\n",
        "output_weights = tiny_model.fc_out.weight.detach().numpy().tolist()\n",
        "\n",
        "# Update the GGUF-like structure with the extracted weights\n",
        "gguf_data = {\n",
        "    \"nodes\": {\n",
        "        \"embedding\": {\n",
        "            \"type\": \"embedding\",\n",
        "            \"input_dim\": 10,\n",
        "            \"output_dim\": 5,\n",
        "            \"weights\": embedding_weights\n",
        "        },\n",
        "        \"attention\": {\n",
        "            \"type\": \"self_attention\",\n",
        "            \"input_dim\": 5,\n",
        "            \"num_heads\": 1,\n",
        "            \"weights\": attention_weights\n",
        "        },\n",
        "        \"output\": {\n",
        "            \"type\": \"linear\",\n",
        "            \"input_dim\": 5,\n",
        "            \"output_dim\": 10,\n",
        "            \"weights\": output_weights\n",
        "        }\n",
        "    },\n",
        "    \"edges\": {\n",
        "        \"embedding_to_attention\": {\"from\": \"embedding\", \"to\": \"attention\"},\n",
        "        \"attention_to_output\": {\"from\": \"attention\", \"to\": \"output\"}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save the updated GGUF-like TOML structure with weights\n",
        "with open(\"tiny_model_gguf_with_weights.toml\", \"w\") as f:\n",
        "    toml.dump(gguf_data, f)\n",
        "\n",
        "# Move the updated TOML file for download\n",
        "import shutil\n",
        "shutil.move(\"tiny_model_gguf_with_weights.toml\", \"/mnt/data/tiny_model_gguf_with_weights.toml\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "go_Igza0HE81",
        "outputId": "0063a85c-ecb6-4bf4-ada6-68410e6a4203"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-f64ffac39e12>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  tiny_model.load_state_dict(torch.load('/mnt/data/tiny_transformer.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/mnt/data/tiny_model_gguf_with_weights.toml'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# GGUF-like JSON structure\n",
        "gguf_data = {\n",
        "    \"nodes\": {\n",
        "        \"embedding\": {\n",
        "            \"type\": \"embedding\",\n",
        "            \"input_dim\": 10,\n",
        "            \"output_dim\": 5,\n",
        "            \"weights\": embedding_weights\n",
        "        },\n",
        "        \"attention\": {\n",
        "            \"type\": \"self_attention\",\n",
        "            \"input_dim\": 5,\n",
        "            \"num_heads\": 1,\n",
        "            \"weights\": attention_weights\n",
        "        },\n",
        "        \"output\": {\n",
        "            \"type\": \"linear\",\n",
        "            \"input_dim\": 5,\n",
        "            \"output_dim\": 10,\n",
        "            \"weights\": output_weights\n",
        "        }\n",
        "    },\n",
        "    \"edges\": {\n",
        "        \"embedding_to_attention\": {\"from\": \"embedding\", \"to\": \"attention\"},\n",
        "        \"attention_to_output\": {\"from\": \"attention\", \"to\": \"output\"}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save the JSON file to a GGUF-like format for testing\n",
        "with open(\"/mnt/data/tiny_model_gguf.json\", \"w\") as f:\n",
        "    json.dump(gguf_data, f)\n",
        "\n",
        "# Output the confirmation of file creation\n",
        "\"GGUF-like JSON model saved.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "h-RzbaEXJFgb",
        "outputId": "dd93faef-8a3c-4e34-d591-f15d063a6b9f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GGUF-like JSON model saved.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import struct\n",
        "\n",
        "# Define the metadata for the tiny model\n",
        "tiny_model_metadata = {\n",
        "    'Model_Architecture': 'TinyTransformer',\n",
        "    'Context_Length': 5,  # Sequence length is 5\n",
        "    'Embedding_Length': 5,  # Embedding dimension is 5\n",
        "    'Block_Count': 1,  # One transformer block\n",
        "    'Feed_Forward_Layer_Size': 5,  # Feed-forward layer size\n",
        "    'RoPE_Dimension_Count': 5,  # RoPE dimension count\n",
        "    'Attention_Head_Count': 1,  # One attention head\n",
        "    'Layer_Norm_Epsilon': 1e-5,  # Epsilon for layer norm\n",
        "    'RoPE_Frequency_Base': 10000,  # RoPE frequency base\n",
        "    'File_Type': 2  # Custom file type\n",
        "}\n",
        "\n",
        "# Step 1: Vocabulary section (token-to-ID mapping)\n",
        "vocab = {\n",
        "    \"vocab\": {str(i): f\"token_{i}\" for i in range(10)}\n",
        "}\n",
        "\n",
        "# Step 2: Extract model weights from the tiny model\n",
        "tiny_model = TinyTransformer(vocab_size=10, hidden_size=5, num_attention_heads=1, num_hidden_layers=1, intermediate_size=5)\n",
        "tiny_model.load_state_dict(torch.load('/mnt/data/tiny_transformer.pth'))\n",
        "\n",
        "embedding_weights = tiny_model.embeddings.weight.detach().numpy().tolist()\n",
        "attention_weights = tiny_model.encoder.layers[0].self_attn.in_proj_weight.detach().numpy().tolist()\n",
        "feedforward_weights = tiny_model.encoder.layers[0].linear1.weight.detach().numpy().tolist()\n",
        "output_weights = tiny_model.fc_out.weight.detach().numpy().tolist()\n",
        "\n",
        "# Step 3: Serialize weights into a binary format (mimicking GGUF large model storage)\n",
        "with open(\"/mnt/data/tiny_model_weights.bin\", \"wb\") as f:\n",
        "    # Write embedding weights\n",
        "    for weight in embedding_weights:\n",
        "        f.write(struct.pack('f'*len(weight), *weight))\n",
        "    # Write attention weights\n",
        "    for weight in attention_weights:\n",
        "        f.write(struct.pack('f'*len(weight), *weight))\n",
        "    # Write feed-forward weights\n",
        "    for weight in feedforward_weights:\n",
        "        f.write(struct.pack('f'*len(weight), *weight))\n",
        "    # Write output layer weights\n",
        "    for weight in output_weights:\n",
        "        f.write(struct.pack('f'*len(weight), *weight))\n",
        "\n",
        "# Step 4: Define GGUF-like structure including references to the binary file\n",
        "gguf_data = {\n",
        "    \"metadata\": tiny_model_metadata,\n",
        "    \"vocab\": vocab,\n",
        "    \"nodes\": {\n",
        "        \"embedding\": {\n",
        "            \"type\": \"embedding\",\n",
        "            \"input_dim\": 10,\n",
        "            \"output_dim\": 5,\n",
        "            \"weights\": \"binary: tiny_model_weights.bin\"\n",
        "        },\n",
        "        \"attention\": {\n",
        "            \"type\": \"self_attention\",\n",
        "            \"input_dim\": 5,\n",
        "            \"num_heads\": 1,\n",
        "            \"weights\": \"binary: tiny_model_weights.bin\"\n",
        "        },\n",
        "        \"feedforward\": {\n",
        "            \"type\": \"feedforward\",\n",
        "            \"input_dim\": 5,\n",
        "            \"output_dim\": 5,\n",
        "            \"weights\": \"binary: tiny_model_weights.bin\"\n",
        "        },\n",
        "        \"output\": {\n",
        "            \"type\": \"linear\",\n",
        "            \"input_dim\": 5,\n",
        "            \"output_dim\": 10,\n",
        "            \"weights\": \"binary: tiny_model_weights.bin\"\n",
        "        }\n",
        "    },\n",
        "    \"edges\": {\n",
        "        \"embedding_to_attention\": {\"from\": \"embedding\", \"to\": \"attention\"},\n",
        "        \"attention_to_feedforward\": {\"from\": \"attention\", \"to\": \"feedforward\"},\n",
        "        \"feedforward_to_output\": {\"from\": \"feedforward\", \"to\": \"output\"}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Step 5: Save the GGUF-like JSON structure\n",
        "with open(\"/mnt/data/tiny_model_gguf_full.json\", \"w\") as f:\n",
        "    json.dump(gguf_data, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhzE65sQLTIi",
        "outputId": "2f1d8ac5-ce70-4d96-c54d-dfa6139c6db0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-52e087279818>:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  tiny_model.load_state_dict(torch.load('/mnt/data/tiny_transformer.pth'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import struct\n",
        "\n",
        "# Step 1: Prepare Metadata, Vocabulary, and Weights\n",
        "metadata = {\n",
        "    'Model_Architecture': 'TinyTransformer',\n",
        "    'Context_Length': 5,  # Sequence length is 5\n",
        "    'Embedding_Length': 5,  # Embedding dimension is 5\n",
        "    'Block_Count': 1,  # One transformer block\n",
        "    'Feed_Forward_Layer_Size': 5,  # Feed-forward layer size\n",
        "    'RoPE_Dimension_Count': 5,  # RoPE dimension count\n",
        "    'Attention_Head_Count': 1,  # One attention head\n",
        "    'Layer_Norm_Epsilon': 1e-5,  # Epsilon for layer norm\n",
        "    'RoPE_Frequency_Base': 10000,  # RoPE frequency base\n",
        "    'File_Type': 2  # Custom file type\n",
        "}\n",
        "\n",
        "vocab = {str(i): f\"token_{i}\" for i in range(10)}\n",
        "\n",
        "# Load the model weights\n",
        "tiny_model = TinyTransformer(vocab_size=10, hidden_size=5, num_attention_heads=1, num_hidden_layers=1, intermediate_size=5)\n",
        "tiny_model.load_state_dict(torch.load('/mnt/data/tiny_transformer.pth'))\n",
        "\n",
        "embedding_weights = tiny_model.embeddings.weight.detach().numpy().tolist()\n",
        "attention_weights = tiny_model.encoder.layers[0].self_attn.in_proj_weight.detach().numpy().tolist()\n",
        "feedforward_weights = tiny_model.encoder.layers[0].linear1.weight.detach().numpy().tolist()\n",
        "output_weights = tiny_model.fc_out.weight.detach().numpy().tolist()\n",
        "\n",
        "# Step 2: Serialize Everything into a Single GGUF File\n",
        "with open(\"/mnt/data/tiny_model.gguf\", \"wb\") as f:\n",
        "    # Write Metadata (Header)\n",
        "    for key, value in metadata.items():\n",
        "        f.write(struct.pack('100s', key.encode()))  # Key\n",
        "        if isinstance(value, float):\n",
        "            f.write(struct.pack('f', value))  # Value as float\n",
        "        elif isinstance(value, int):\n",
        "            f.write(struct.pack('i', value))  # Value as int\n",
        "        else:\n",
        "            f.write(struct.pack('100s', str(value).encode()))  # Value as string\n",
        "\n",
        "    # Write Vocabulary\n",
        "    f.write(struct.pack('100s', \"Vocabulary\".encode()))\n",
        "    for token_id, token in vocab.items():\n",
        "        f.write(struct.pack('i', int(token_id)))  # Token ID\n",
        "        f.write(struct.pack('100s', token.encode()))  # Token\n",
        "\n",
        "    # Write Embedding Weights\n",
        "    f.write(struct.pack('100s', \"Embedding_Weights\".encode()))\n",
        "    for weight in embedding_weights:\n",
        "        f.write(struct.pack('f'*len(weight), *weight))\n",
        "\n",
        "    # Write Attention Weights\n",
        "    f.write(struct.pack('100s', \"Attention_Weights\".encode()))\n",
        "    for weight in attention_weights:\n",
        "        f.write(struct.pack('f'*len(weight), *weight))\n",
        "\n",
        "    # Write Feedforward Weights\n",
        "    f.write(struct.pack('100s', \"Feedforward_Weights\".encode()))\n",
        "    for weight in feedforward_weights:\n",
        "        f.write(struct.pack('f'*len(weight), *weight))\n",
        "\n",
        "    # Write Output Layer Weights\n",
        "    f.write(struct.pack('100s', \"Output_Weights\".encode()))\n",
        "    for weight in output_weights:\n",
        "        f.write(struct.pack('f'*len(weight), *weight))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jr4OZJlXMoUU",
        "outputId": "0dbdc06f-d6d2-45bd-8582-99650b1a1dcd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-7bad67e8af36>:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  tiny_model.load_state_dict(torch.load('/mnt/data/tiny_transformer.pth'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's attempt to parse and inspect the content of the GGUF-like file\n",
        "gguf_file_path = '/mnt/data/tiny_model.gguf'\n",
        "\n",
        "# Reading and displaying the binary content from the GGUF-like file\n",
        "with open(gguf_file_path, 'rb') as f:\n",
        "    gguf_content = f.read()\n",
        "\n",
        "# Display the first 1024 bytes of the GGUF file for inspection\n",
        "gguf_content[:1024]  # Show the first portion of the binary file for inspection\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9KZXNUSNXZL",
        "outputId": "93db9715-dcf7-4a90-a78f-490296de2685"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b\"Model_Architecture\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00TinyTransformer\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Context_Length\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00Embedding_Length\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00Block_Count\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00Feed_Forward_Layer_Size\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00RoPE_Dimension_Count\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00Attention_Head_Count\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00Layer_Norm_Epsilon\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xac\\xc5'7RoPE_Frequency_Base\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\""
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import struct\n",
        "import json\n",
        "\n",
        "def parse_gguf(gguf_file_path):\n",
        "    with open(gguf_file_path, 'rb') as f:\n",
        "        # Step 1: Initialize the dictionary to store parsed GGUF content\n",
        "        gguf_data = {}\n",
        "\n",
        "        while True:\n",
        "            # Step 2: Read the next key (assuming keys are 100-byte strings in this example)\n",
        "            key_bytes = f.read(100)\n",
        "            if not key_bytes:\n",
        "                break  # End of file\n",
        "            key = key_bytes.decode('utf-8').strip('\\x00')\n",
        "\n",
        "            # Step 3: Based on the key, read the corresponding value\n",
        "            if key == \"Model_Architecture\":\n",
        "                value_bytes = f.read(100)\n",
        "                value = value_bytes.decode('utf-8').strip('\\x00')\n",
        "            elif key in [\"Context_Length\", \"Embedding_Length\", \"Block_Count\", \"Feed_Forward_Layer_Size\", \"RoPE_Dimension_Count\", \"Attention_Head_Count\"]:\n",
        "                value_bytes = f.read(4)\n",
        "                value = struct.unpack('i', value_bytes)[0]  # Read as an integer\n",
        "            elif key == \"Layer_Norm_Epsilon\":\n",
        "                value_bytes = f.read(4)\n",
        "                value = struct.unpack('f', value_bytes)[0]  # Read as a float\n",
        "            elif key == \"RoPE_Frequency_Base\":\n",
        "                value_bytes = f.read(4)\n",
        "                value = struct.unpack('f', value_bytes)[0]  # Read as a float\n",
        "            else:\n",
        "                # For unknown keys, skip to next key (e.g., handling weights)\n",
        "                continue\n",
        "\n",
        "            # Step 4: Add the key-value pair to the dictionary\n",
        "            gguf_data[key] = value\n",
        "\n",
        "        return gguf_data\n",
        "\n",
        "# Step 5: Parse the GGUF file and convert to JSON\n",
        "gguf_file_path = '/mnt/data/tiny_model.gguf'\n",
        "gguf_data = parse_gguf(gguf_file_path)\n",
        "\n",
        "# Step 6: Serialize to JSON for readability\n",
        "gguf_json = json.dumps(gguf_data, indent=4)\n",
        "print(gguf_json)\n",
        "\n",
        "# Optional: Save the JSON to a file\n",
        "with open(\"/mnt/data/tiny_model_gguf_parsed.json\", \"w\") as json_file:\n",
        "    json_file.write(gguf_json)\n",
        "\n",
        "# Confirm successful operation\n",
        "\"GGUF successfully parsed and saved as JSON.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "5wC1dtIyP_7X",
        "outputId": "0b32d664-83f3-429b-f355-68b084ffc50d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0xd0 in position 44: invalid continuation byte",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-30f4b1bad17e>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Step 5: Parse the GGUF file and convert to JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mgguf_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/mnt/data/tiny_model.gguf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mgguf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_gguf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgguf_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Step 6: Serialize to JSON for readability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-30f4b1bad17e>\u001b[0m in \u001b[0;36mparse_gguf\u001b[0;34m(gguf_file_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey_bytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# End of file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_bytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\x00'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# Step 3: Based on the key, read the corresponding value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd0 in position 44: invalid continuation byte"
          ]
        }
      ]
    },
    {
      "source": [
        "import struct\n",
        "import json\n",
        "\n",
        "def parse_gguf(gguf_file_path):\n",
        "    with open(gguf_file_path, 'rb') as f:\n",
        "        # Step 1: Initialize the dictionary to store parsed GGUF content\n",
        "        gguf_data = {}\n",
        "\n",
        "        while True:\n",
        "            # Step 2: Read the next key (assuming keys are 100-byte strings in this example)\n",
        "            key_bytes = f.read(100)\n",
        "            if not key_bytes:\n",
        "                break  # End of file\n",
        "\n",
        "            # Decode key_bytes only if it represents a string\n",
        "            try:\n",
        "                key = key_bytes.decode('utf-8').strip('\\x00')\n",
        "            except UnicodeDecodeError:\n",
        "                # Handle cases where key_bytes is not a string (e.g., binary data)\n",
        "                key = key_bytes\n",
        "\n",
        "            # Step 3: Based on the key, read the corresponding value\n",
        "            if key == \"Model_Architecture\":\n",
        "                value_bytes = f.read(100)\n",
        "                value = value_bytes.decode('utf-8').strip('\\x00')\n",
        "            elif key in [\"Context_Length\", \"Embedding_Length\", \"Block_Count\", \"Feed_Forward_Layer_Size\", \"RoPE_Dimension_Count\", \"Attention_Head_Count\"]:\n",
        "                value_bytes = f.read(4)\n",
        "                value = struct.unpack('i', value_bytes)[0]  # Read as an integer\n",
        "            elif key == \"Layer_Norm_Epsilon\":\n",
        "                value_bytes = f.read(4)\n",
        "                value = struct.unpack('f', value_bytes)[0]  # Read as a float\n",
        "            elif key == \"RoPE_Frequency_Base\":\n",
        "                value_bytes = f.read(4)\n",
        "                value = struct.unpack('f', value_bytes)[0]  # Read as a float\n",
        "            else:\n",
        "                # For unknown keys, skip to next key (e.g., handling weights)\n",
        "                continue\n",
        "\n",
        "            # Step 4: Add the key-value pair to the dictionary\n",
        "            gguf_data[key] = value\n",
        "\n",
        "        return gguf_data\n",
        "\n",
        "# Step 5: Parse the GGUF file and convert to JSON\n",
        "gguf_file_path = '/mnt/data/tiny_model.gguf'\n",
        "gguf_data = parse_gguf(gguf_file_path)\n",
        "\n",
        "# Step 6: Serialize to JSON for readability\n",
        "gguf_json = json.dumps(gguf_data, indent=4)\n",
        "print(gguf_json)\n",
        "\n",
        "# Optional: Save the JSON to a file\n",
        "with open(\"/mnt/data/tiny_model_gguf_parsed.json\", \"w\") as json_file:\n",
        "    json_file.write(gguf_json)\n",
        "\n",
        "# Confirm successful operation\n",
        "\"GGUF successfully parsed and saved as JSON.\""
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "Xb4OK9_YQKMd",
        "outputId": "93a63b52-4abe-43f0-a8d3-d4424aa32944"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"Model_Architecture\": \"TinyTransformer\",\n",
            "    \"Context_Length\": 5,\n",
            "    \"Embedding_Length\": 5,\n",
            "    \"Block_Count\": 1,\n",
            "    \"Feed_Forward_Layer_Size\": 5,\n",
            "    \"RoPE_Dimension_Count\": 5,\n",
            "    \"Attention_Head_Count\": 1,\n",
            "    \"Layer_Norm_Epsilon\": 9.999999747378752e-06,\n",
            "    \"RoPE_Frequency_Base\": 1.401298464324817e-41\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GGUF successfully parsed and saved as JSON.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import struct\n",
        "import json\n",
        "\n",
        "def parse_gguf(gguf_file_path):\n",
        "    with open(gguf_file_path, 'rb') as f:\n",
        "        # Initialize the dictionary to store parsed GGUF content\n",
        "        gguf_data = {\n",
        "            \"metadata\": {},\n",
        "            \"vocabulary\": {},\n",
        "            \"weights\": {\n",
        "                \"embedding\": [],\n",
        "                \"attention\": [],\n",
        "                \"feedforward\": [],\n",
        "                \"output\": []\n",
        "            }\n",
        "        }\n",
        "\n",
        "        while True:\n",
        "            # Read the next key (assuming keys are 100-byte strings in this example)\n",
        "            key_bytes = f.read(100)\n",
        "            if not key_bytes:\n",
        "                break  # End of file\n",
        "            key = key_bytes.decode('utf-8').strip('\\x00')\n",
        "\n",
        "            # Parse based on the key\n",
        "            if key == \"Model_Architecture\":\n",
        "                value_bytes = f.read(100)\n",
        "                value = value_bytes.decode('utf-8').strip('\\x00')\n",
        "                gguf_data[\"metadata\"][key] = value\n",
        "\n",
        "            elif key in [\"Context_Length\", \"Embedding_Length\", \"Block_Count\", \"Feed_Forward_Layer_Size\", \"RoPE_Dimension_Count\", \"Attention_Head_Count\"]:\n",
        "                value_bytes = f.read(4)\n",
        "                value = struct.unpack('i', value_bytes)[0]\n",
        "                gguf_data[\"metadata\"][key] = value\n",
        "\n",
        "            elif key in [\"Layer_Norm_Epsilon\", \"RoPE_Frequency_Base\"]:\n",
        "                value_bytes = f.read(4)\n",
        "                value = struct.unpack('f', value_bytes)[0]\n",
        "                gguf_data[\"metadata\"][key] = value\n",
        "\n",
        "            elif key == \"Vocabulary\":\n",
        "                # Parse vocabulary (assuming token ID and token pairs)\n",
        "                vocab_size_bytes = f.read(4)\n",
        "                vocab_size = struct.unpack('i', vocab_size_bytes)[0]\n",
        "                for _ in range(vocab_size):\n",
        "                    token_id_bytes = f.read(4)\n",
        "                    token_id = struct.unpack('i', token_id_bytes)[0]\n",
        "                    token_bytes = f.read(100)\n",
        "                    token = token_bytes.decode('utf-8').strip('\\x00')\n",
        "                    gguf_data[\"vocabulary\"][token_id] = token\n",
        "\n",
        "            elif key == \"Embedding_Weights\":\n",
        "                # Parse embedding weights (assuming 5x5 matrix for embedding)\n",
        "                for _ in range(5):\n",
        "                    weight_bytes = f.read(4 * 5)  # 5 floats for each embedding row\n",
        "                    weights = struct.unpack('f' * 5, weight_bytes)\n",
        "                    gguf_data[\"weights\"][\"embedding\"].append(weights)\n",
        "\n",
        "            elif key == \"Attention_Weights\":\n",
        "                # Parse attention weights\n",
        "                for _ in range(5):  # Assuming attention weights are also 5x5 for simplicity\n",
        "                    weight_bytes = f.read(4 * 5)\n",
        "                    weights = struct.unpack('f' * 5, weight_bytes)\n",
        "                    gguf_data[\"weights\"][\"attention\"].append(weights)\n",
        "\n",
        "            elif key == \"Feedforward_Weights\":\n",
        "                # Parse feed-forward layer weights\n",
        "                for _ in range(5):  # Assuming 5x5 size for the feedforward layer\n",
        "                    weight_bytes = f.read(4 * 5)\n",
        "                    weights = struct.unpack('f' * 5, weight_bytes)\n",
        "                    gguf_data[\"weights\"][\"feedforward\"].append(weights)\n",
        "\n",
        "            elif key == \"Output_Weights\":\n",
        "                # Parse output layer weights (5x10)\n",
        "                for _ in range(10):  # Output layer is 5x10\n",
        "                    weight_bytes = f.read(4 * 5)\n",
        "                    weights = struct.unpack('f' * 5, weight_bytes)\n",
        "                    gguf_data[\"weights\"][\"output\"].append(weights)\n",
        "\n",
        "        return gguf_data\n",
        "\n",
        "# Parse the GGUF file and convert it to JSON\n",
        "gguf_file_path = '/mnt/data/tiny_model.gguf'\n",
        "gguf_data = parse_gguf(gguf_file_path)\n",
        "\n",
        "# Convert to JSON for readability\n",
        "gguf_json = json.dumps(gguf_data, indent=4)\n",
        "print(gguf_json)\n",
        "\n",
        "# Save the JSON to a file\n",
        "with open(\"/mnt/data/tiny_model_gguf_full_parsed.json\", \"w\") as json_file:\n",
        "    json_file.write(gguf_json)\n",
        "\n",
        "# Confirm the successful parsing and saving of the file\n",
        "\"GGUF fully parsed and saved as JSON.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "Wvo41aN9RMQF",
        "outputId": "c143f855-22b4-4cf2-cc18-1d06182a390f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0xeb in position 1: invalid continuation byte",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-d07a3a517453>\u001b[0m in \u001b[0;36m<cell line: 84>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# Parse the GGUF file and convert it to JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mgguf_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/mnt/data/tiny_model.gguf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mgguf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_gguf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgguf_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# Convert to JSON for readability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-d07a3a517453>\u001b[0m in \u001b[0;36mparse_gguf\u001b[0;34m(gguf_file_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey_bytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# End of file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_bytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\x00'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# Parse based on the key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xeb in position 1: invalid continuation byte"
          ]
        }
      ]
    },
    {
      "source": [
        "import struct\n",
        "import json\n",
        "\n",
        "def parse_gguf(gguf_file_path):\n",
        "    with open(gguf_file_path, 'rb') as f:\n",
        "        # Initialize the dictionary to store parsed GGUF content\n",
        "        gguf_data = {\n",
        "            \"metadata\": {},\n",
        "            \"vocabulary\": {},\n",
        "            \"weights\": {\n",
        "                \"embedding\": [],\n",
        "                \"attention\": [],\n",
        "                \"feedforward\": [],\n",
        "                \"output\": []\n",
        "            }\n",
        "        }\n",
        "\n",
        "        while True:\n",
        "            # Read the next key (assuming keys are 100-byte strings in this example)\n",
        "            key_bytes = f.read(100)\n",
        "            if not key_bytes:\n",
        "                break  # End of file\n",
        "\n",
        "            # Attempt to decode key_bytes as UTF-8, otherwise handle as bytes\n",
        "            try:\n",
        "                key = key_bytes.decode('utf-8').strip('\\x00')\n",
        "            except UnicodeDecodeError:\n",
        "                key = key_bytes\n",
        "\n",
        "            # Parse based on the key\n",
        "            if key == \"Model_Architecture\":\n",
        "                value_bytes = f.read(100)\n",
        "                value = value_bytes.decode('utf-8').strip('\\x00')\n",
        "                gguf_data[\"metadata\"][key] = value\n",
        "\n",
        "            elif key in [\"Context_Length\", \"Embedding_Length\", \"Block_Count\", \"Feed_Forward_Layer_Size\", \"RoPE_Dimension_Count\", \"Attention_Head_Count\"]:\n",
        "                value_bytes = f.read(4)\n",
        "                value = struct.unpack('i', value_bytes)[0]\n",
        "                gguf_data[\"metadata\"][key] = value\n",
        "\n",
        "            elif key in [\"Layer_Norm_Epsilon\", \"RoPE_Frequency_Base\"]:\n",
        "                value_bytes = f.read(4)\n",
        "                value = struct.unpack('f', value_bytes)[0]\n",
        "                gguf_data[\"metadata\"][key] = value\n",
        "\n",
        "            elif key == \"Vocabulary\":\n",
        "                # Parse vocabulary (assuming token ID and token pairs)\n",
        "                vocab_size_bytes = f.read(4)\n",
        "                vocab_size = struct.unpack('i', vocab_size_bytes)[0]\n",
        "                for _ in range(vocab_size):\n",
        "                    token_id_bytes = f.read(4)\n",
        "                    token_id = struct.unpack('i', token_id_bytes)[0]\n",
        "                    token_bytes = f.read(100)\n",
        "                    token = token_bytes.decode('utf-8').strip('\\x00')\n",
        "                    gguf_data[\"vocabulary\"][token_id] = token\n",
        "\n",
        "            elif key == \"Embedding_Weights\":\n",
        "                # Parse embedding weights (assuming 5x5 matrix for embedding)\n",
        "                for _ in range(5):\n",
        "                    weight_bytes = f.read(4 * 5)  # 5 floats for each embedding row\n",
        "                    weights = struct.unpack('f' * 5, weight_bytes)\n",
        "                    gguf_data[\"weights\"][\"embedding\"].append(weights)\n",
        "\n",
        "            elif key == \"Attention_Weights\":\n",
        "                # Parse attention weights\n",
        "                for _ in range(5):  # Assuming attention weights are also 5x5 for simplicity\n",
        "                    weight_bytes = f.read(4 * 5)\n",
        "                    weights = struct.unpack('f' * 5, weight_bytes)\n",
        "                    gguf_data[\"weights\"][\"attention\"].append(weights)\n",
        "\n",
        "            elif key == \"Feedforward_Weights\":\n",
        "                # Parse feed-forward layer weights\n",
        "                for _ in range(5):  # Assuming 5x5 size for the feedforward layer\n",
        "                    weight_bytes = f.read(4 * 5)\n",
        "                    weights = struct.unpack('f' * 5, weight_bytes)\n",
        "gguf_data[\"weights\"][\"feedforward\"].append(weights)\n",
        "\n",
        "            elif key == \"Output_\":"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "IEsTe8_gRfdZ",
        "outputId": "5f3d51e3-923e-4379-a26a-dd8b4b1693b3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-24-813c221a4c49>, line 78)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-813c221a4c49>\"\u001b[0;36m, line \u001b[0;32m78\u001b[0m\n\u001b[0;31m    elif key == \"Output_\":\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import struct\n",
        "import json\n",
        "\n",
        "def parse_gguf(gguf_file_path):\n",
        "    with open(gguf_file_path, 'rb') as f:\n",
        "        # Initialize the dictionary to store parsed GGUF content\n",
        "        gguf_data = {\n",
        "            \"metadata\": {},\n",
        "            \"vocabulary\": {},\n",
        "            \"weights\": {\n",
        "                \"embedding\": [],\n",
        "                \"attention\": [],\n",
        "                \"feedforward\": [],\n",
        "                \"output\": []\n",
        "            }\n",
        "        }\n",
        "\n",
        "        while True:\n",
        "            # Read the next key (assuming keys are 100-byte strings in this example)\n",
        "            key_bytes = f.read(100)\n",
        "            if not key_bytes:\n",
        "                break  # End of file\n",
        "\n",
        "            # Attempt to decode key_bytes as UTF-8, otherwise handle as bytes\n",
        "            try:\n",
        "                key = key_bytes.decode('utf-8').strip('\\x00')\n",
        "            except UnicodeDecodeError:\n",
        "                key = key_bytes\n",
        "\n",
        "            # Parse based on the key\n",
        "            if key == \"Model_Architecture\":\n",
        "                value_bytes = f.read(100)\n",
        "                value = value_bytes.decode('utf-8').strip('\\x00')\n",
        "                gguf_data[\"metadata\"][key] = value\n",
        "\n",
        "            elif key in [\"Context_Length\", \"Embedding_Length\", \"Block_Count\", \"Feed_Forward_Layer_Size\", \"RoPE_Dimension_Count\", \"Attention_Head_Count\"]:\n",
        "                value_bytes = f.read(4)\n",
        "                value = struct.unpack('i', value_bytes)[0]\n",
        "                gguf_data[\"metadata\"][key] = value\n",
        "\n",
        "            elif key in [\"Layer_Norm_Epsilon\", \"RoPE_Frequency_Base\"]:\n",
        "                value_bytes = f.read(4)\n",
        "                value = struct.unpack('f', value_bytes)[0]\n",
        "                gguf_data[\"metadata\"][key] = value\n",
        "\n",
        "            elif key == \"Vocabulary\":\n",
        "                # Parse vocabulary (assuming token ID and token pairs)\n",
        "                vocab_size_bytes = f.read(4)\n",
        "                vocab_size = struct.unpack('i', vocab_size_bytes)[0]\n",
        "                for _ in range(vocab_size):\n",
        "                    token_id_bytes = f.read(4)\n",
        "                    token_id = struct.unpack('i', token_id_bytes)[0]\n",
        "                    token_bytes = f.read(100)\n",
        "                    token = token_bytes.decode('utf-8').strip('\\x00')\n",
        "                    gguf_data[\"vocabulary\"][token_id] = token\n",
        "\n",
        "            elif key == \"Embedding_Weights\":\n",
        "                # Parse embedding weights (assuming 5x5 matrix for embedding)\n",
        "                for _ in range(5):\n",
        "                    weight_bytes = f.read(4 * 5)  # 5 floats for each embedding row\n",
        "                    weights = struct.unpack('f' * 5, weight_bytes)\n",
        "                    gguf_data[\"weights\"][\"embedding\"].append(weights)\n",
        "\n",
        "            elif key == \"Attention_Weights\":\n",
        "                # Parse attention weights\n",
        "                for _ in range(5):  # Assuming attention weights are also 5x5 for simplicity\n",
        "                    weight_bytes = f.read(4 * 5)\n",
        "                    weights = struct.unpack('f' * 5, weight_bytes)\n",
        "                    gguf_data[\"weights\"][\"attention\"].append(weights)\n",
        "\n",
        "            elif key == \"Feedforward_Weights\":\n",
        "                # Parse feed-forward layer weights\n",
        "                for _ in range(5):  # Assuming 5x5 size for the feedforward layer\n",
        "                    weight_bytes = f.read(4 * 5)\n",
        "                    weights = struct.unpack('f' * 5, weight_bytes)\n",
        "                    gguf_data[\"weights\"][\"feedforward\"].append(weights)\n",
        "\n",
        "            elif key == \"Output_Weights\": # Fixed indentation\n",
        "                # Parse output layer weights (5x10)\n",
        "                for _ in range(10):  # Output layer is 5x10\n",
        "                    weight_bytes = f.read(4 * 5)\n",
        "                    weights = struct.unpack('f' * 5, weight_bytes)\n",
        "                    gguf_data[\"weights\"][\"output\"].append(weights)\n",
        "\n",
        "        return gguf_data\n",
        "\n",
        "# Parse the GGUF file and convert it to JSON\n",
        "gguf_file_path = '/mnt/data/tiny_model.gguf'\n",
        "gguf_data = parse_gguf(gguf_file_path)\n",
        "\n",
        "# Convert to JSON for readability\n",
        "gguf_json = json.dumps(gguf_data, indent=4)\n",
        "print(gguf_json)\n",
        "\n",
        "# Save the JSON to a file\n",
        "with open(\"/mnt/data/tiny_model_gguf_full_parsed.json\", \"w\") as json_file:\n",
        "    json_file.write(gguf_json)\n",
        "\n",
        "# Confirm the successful parsing and saving of the file\n",
        "\"GGUF fully parsed and saved as JSON.\"\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HYW9zVImR0wC",
        "outputId": "4487841f-a641-4551-cb0b-8eae6084144a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"metadata\": {\n",
            "        \"Model_Architecture\": \"TinyTransformer\",\n",
            "        \"Context_Length\": 5,\n",
            "        \"Embedding_Length\": 5,\n",
            "        \"Block_Count\": 1,\n",
            "        \"Feed_Forward_Layer_Size\": 5,\n",
            "        \"RoPE_Dimension_Count\": 5,\n",
            "        \"Attention_Head_Count\": 1,\n",
            "        \"Layer_Norm_Epsilon\": 9.999999747378752e-06,\n",
            "        \"RoPE_Frequency_Base\": 1.401298464324817e-41\n",
            "    },\n",
            "    \"vocabulary\": {},\n",
            "    \"weights\": {\n",
            "        \"embedding\": [\n",
            "            [\n",
            "                0.0,\n",
            "                0.0,\n",
            "                0.0,\n",
            "                0.0,\n",
            "                0.0\n",
            "            ],\n",
            "            [\n",
            "                0.0,\n",
            "                0.0,\n",
            "                0.0,\n",
            "                0.0,\n",
            "                0.0\n",
            "            ],\n",
            "            [\n",
            "                0.0,\n",
            "                0.9347047805786133,\n",
            "                0.07833842188119888,\n",
            "                -0.1573018878698349,\n",
            "                -0.37725022435188293\n",
            "            ],\n",
            "            [\n",
            "                -1.4977898597717285,\n",
            "                -1.1747392416000366,\n",
            "                -0.5798462629318237,\n",
            "                -0.9467189908027649,\n",
            "                0.22410157322883606\n",
            "            ],\n",
            "            [\n",
            "                0.19144682586193085,\n",
            "                -0.3395657539367676,\n",
            "                0.648118793964386,\n",
            "                -0.17575573921203613,\n",
            "                -1.840567946434021\n",
            "            ]\n",
            "        ],\n",
            "        \"attention\": [],\n",
            "        \"feedforward\": [],\n",
            "        \"output\": []\n",
            "    }\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GGUF fully parsed and saved as JSON.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Assuming we have the tiny transformer model class defined earlier\n",
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size=10, hidden_size=5, num_attention_heads=1, num_hidden_layers=1, intermediate_size=5):\n",
        "        super(TinyTransformer, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        # Transformer encoder layer with batch_first=True for better performance\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_attention_heads, dim_feedforward=intermediate_size, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_hidden_layers)\n",
        "\n",
        "        # Output layer (simple linear classifier for vocab prediction)\n",
        "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # Embed the input tokens\n",
        "        embeddings = self.embeddings(input_ids)\n",
        "\n",
        "        # Pass through the transformer encoder\n",
        "        encoder_output = self.encoder(embeddings)\n",
        "\n",
        "        # Generate predictions\n",
        "        logits = self.fc_out(encoder_output)\n",
        "        return logits\n",
        "\n",
        "# Instantiate the tiny model\n",
        "tiny_model = TinyTransformer(vocab_size=10, hidden_size=5, num_attention_heads=1, num_hidden_layers=1, intermediate_size=5)\n",
        "\n",
        "# Load the weights into the model (assuming you have the state dict from your previously saved model)\n",
        "tiny_model.load_state_dict(torch.load('/mnt/data/tiny_transformer.pth'))\n",
        "\n",
        "# Simulate input tokens for a query\n",
        "# Token IDs from 0 to 9 (simulating a query using the small vocabulary)\n",
        "input_tokens = torch.tensor([[0, 1, 2, 3, 4]])\n",
        "\n",
        "# Pass the input tokens through the model\n",
        "output_logits = tiny_model(input_tokens)\n",
        "\n",
        "# Convert output logits to probabilities (softmax)\n",
        "output_probs = torch.softmax(output_logits, dim=-1)\n",
        "\n",
        "# Get the predicted tokens\n",
        "predicted_tokens = torch.argmax(output_probs, dim=-1)\n",
        "\n",
        "# Convert predicted token IDs back to strings (for example, map 0 -> \"token_0\")\n",
        "vocab = {i: f\"token_{i}\" for i in range(10)}\n",
        "predicted_token_strings = [[vocab[token.item()] for token in sequence] for sequence in predicted_tokens]\n",
        "\n",
        "predicted_token_strings\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4IBCx-FTkHr",
        "outputId": "08247d2e-f81e-43b5-dbf0-e6acb44493e4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "<ipython-input-27-fc60b0fde5c8>:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  tiny_model.load_state_dict(torch.load('/mnt/data/tiny_transformer.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['token_0', 'token_6', 'token_8', 'token_0', 'token_8']]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate the input for the word \"hi\"\n",
        "# Mapping \"h\" -> token_3, \"i\" -> token_1\n",
        "input_tokens_hi = torch.tensor([[3, 1]])\n",
        "\n",
        "# Pass the input tokens through the model\n",
        "output_logits_hi = tiny_model(input_tokens_hi)\n",
        "\n",
        "# Convert output logits to probabilities (softmax)\n",
        "output_probs_hi = torch.softmax(output_logits_hi, dim=-1)\n",
        "\n",
        "# Get the predicted tokens\n",
        "predicted_tokens_hi = torch.argmax(output_probs_hi, dim=-1)\n",
        "\n",
        "# Convert predicted token IDs back to strings\n",
        "predicted_token_strings_hi = [[vocab[token.item()] for token in sequence] for sequence in predicted_tokens_hi]\n",
        "\n",
        "predicted_token_strings_hi  # Display the predicted token sequence for \"hi\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD83yVaYUMni",
        "outputId": "a852620c-9ef9-4d98-e512-1d8b47363e76"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['token_0', 'token_6']]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}